{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "RATES = [1,2,3,4,5]\n",
    "\n",
    "def loadData(nbChunks = 0):  # <= 0 to load all chunks, positive integer to load specific number of chunks\n",
    "    asinToIndex = {}\n",
    "    scoreMatrix = []\n",
    "    booksAsin = []\n",
    "    index = 0\n",
    "    chunks = pd.read_json(\n",
    "        '../Dataset/Books.json', lines=True, chunksize=500000,\n",
    "        typ=\"frame\", orient=\"records\", dtype={\"asin\": str, \"overall\": int, \"unixReviewTime\": int}\n",
    "    )\n",
    "    for chunk in chunks:\n",
    "        for _, item in chunk.iterrows():\n",
    "            asin = item[\"asin\"]\n",
    "            rate = item[\"overall\"]\n",
    "            if (asin not in asinToIndex):\n",
    "                scoreMatrix.append([0,0,0,0,0])\n",
    "                booksAsin.append(asin)\n",
    "                asinToIndex[asin] = index\n",
    "                index += 1\n",
    "            row = asinToIndex[asin]\n",
    "            scoreMatrix[row][rate-1] += 1\n",
    "        nbChunks -= 1\n",
    "        if (nbChunks == 0): break\n",
    "    scoreMatrix = np.array(scoreMatrix)\n",
    "    nbNonAppreciate = scoreMatrix[:,0]+scoreMatrix[:,1]\n",
    "    nbNeutral = scoreMatrix[:,2]\n",
    "    nbAppreciate = scoreMatrix[:,3]+scoreMatrix[:,4]\n",
    "    realClass = np.argmax(np.column_stack((nbNonAppreciate, nbNeutral, nbAppreciate)), axis=1).astype(int)\n",
    "    maxStarVotes = np.argmax(scoreMatrix, axis=1).astype(int)\n",
    "    categoryMatrix = np.column_stack((nbNonAppreciate, nbNeutral, nbAppreciate, maxStarVotes, realClass))\n",
    "    colNames = [f\"{i} ⭐\" for i in RATES]\n",
    "    colNames.extend([\"Depreciation\", \"Neutral\", \"Appreciations\", \"MaxStarVotes\", \"Class\"])\n",
    "    pdBooks = pd.DataFrame(data=np.concatenate((scoreMatrix, categoryMatrix), axis=1), index=booksAsin, columns=colNames)\n",
    "    return pdBooks, scoreMatrix, categoryMatrix, booksAsin, asinToIndex\n",
    "\n",
    "pdBooks, scoreMatrix, categoryMatrix, booksAsin, asinToIndex = loadData()\n",
    "display(pdBooks)\n",
    "\n",
    "def displayDifferentClassesCount(pdBooks):\n",
    "    print(\"Different classes count : \")\n",
    "    classesCount = np.array([\n",
    "        [(pdBooks.Class.values == i).sum(), int(np.round(((pdBooks.Class.values == i).sum()/len(pdBooks.index))*100))] for i in range(0, 3)\n",
    "    ])\n",
    "    display(pd.DataFrame(\n",
    "        data=np.array(classesCount),\n",
    "        index=[ (i+1) * \"⭐\" for i in range(0,3) ],\n",
    "        columns=[\"Count\", \"%\"]\n",
    "    ))\n",
    "\n",
    "displayDifferentClassesCount(pdBooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOKS_SAMPLING_NB = 5000\n",
    "BAYES_QUARTILE_OPT = 0.25\n",
    "colorsList = [\"red\", \"orange\", \"green\"]\n",
    "colors = ListedColormap(colorsList)\n",
    "CRITERIAS = [\"Nb rating\", \"Average Rating\", \"Bayes Rating\", \"Median\", \"STD\", \"Depreciation\", \"Neutral\", \"Appreciations\", \"MaxStarVotes\"]\n",
    "\n",
    "class PlotData:\n",
    "  def __init__(self, inputData, dataXY, dataLabels, title, axisLabels=[]):\n",
    "    self.inputData = inputData\n",
    "    self.dataXY = dataXY\n",
    "    self.dataLabels = dataLabels\n",
    "    self.title = title\n",
    "    self.axisLabels = axisLabels\n",
    "\n",
    "def getMedian(scoreMatrix, nbRating):\n",
    "    def rowMedian(i):\n",
    "        s = 0\n",
    "        for k in sorted([0,1,2,3,4]):\n",
    "            s += scoreMatrix[i,k]\n",
    "            if s > nbRating[i] / 2:\n",
    "                break\n",
    "        return k + 1\n",
    "    medianMat = np.array([ rowMedian(i) for i in range(0, nbRating.size) ])\n",
    "    return medianMat\n",
    "\n",
    "def getBookStats(pdBooks):\n",
    "    scoreMatrix = pdBooks.to_numpy()\n",
    "    starsMatrix = scoreMatrix[:,0:5]\n",
    "    nbRating = np.sum(starsMatrix, axis=1)\n",
    "    avgRatings = (np.average(starsMatrix, axis=1, weights=RATES) * np.sum(RATES)) / nbRating\n",
    "    medianMat = getMedian(starsMatrix, nbRating)\n",
    "    stdRatings = np.std(starsMatrix, axis=1)\n",
    "    confidenceNumber = np.quantile(nbRating, BAYES_QUARTILE_OPT)\n",
    "    arithAverage = np.mean(avgRatings)\n",
    "    bayesAvgRatings = (nbRating * avgRatings + arithAverage * confidenceNumber) / (nbRating + confidenceNumber)\n",
    "    calculatedPropsMat = np.column_stack((nbRating, avgRatings, bayesAvgRatings, medianMat, stdRatings))\n",
    "    return pd.DataFrame(\n",
    "        data=np.concatenate((calculatedPropsMat, scoreMatrix[:,5:]),axis=1),\n",
    "        index=pdBooks.index, \n",
    "        columns=[\"Nb rating\", \"Average Rating\", \"Bayes Rating\", \"Median\", \"STD\", \"Depreciation\", \"Neutral\", \"Appreciations\", \"MaxStarVotes\", \"Class\"]\n",
    "    )\n",
    "\n",
    "pdBooksFeatures = getBookStats(pdBooks.sample(BOOKS_SAMPLING_NB))\n",
    "display(pdBooksFeatures)\n",
    "displayDifferentClassesCount(pdBooksFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLUSTERS = 3\n",
    "CLASSES = [f\"C{i+1}\" for i in range(0, NB_CLUSTERS)]\n",
    "\n",
    "def kmeansClustering(booksData, distFunc, show=False):\n",
    "    # Standardize data before doing kmeans :\n",
    "    data = (booksData - booksData.mean()) / booksData.std()\n",
    "    # As per : https://stackoverflow.com/questions/46409846/using-k-means-with-cosine-similarity-python\n",
    "    if (distFunc == \"cosine\"): data = preprocessing.normalize(data)\n",
    "    kmeans = KMeans(n_clusters=NB_CLUSTERS, random_state=0, max_iter=1000).fit(data)\n",
    "    # K means\n",
    "    if (show):\n",
    "        print(f\"K-means ({distFunc}):\")\n",
    "        display(pd.DataFrame(\n",
    "            data=np.array([ [f\"C{i+1}\" for i in kmeans.labels_] ]),\n",
    "            index=[\"Class\"],\n",
    "            columns=booksData.index\n",
    "        ))\n",
    "        display(pd.DataFrame(\n",
    "            data=np.array([[np.count_nonzero(kmeans.labels_ == i) for i in range(0, NB_CLUSTERS)]]),\n",
    "            index=[\"Number of Classes\"],\n",
    "            columns=CLASSES\n",
    "        ))\n",
    "    return data, kmeans\n",
    "\n",
    "kmeansEucliCoords, kmeansEucliOutput = kmeansClustering(pdBooksFeatures[CRITERIAS], \"euclidean\", True)\n",
    "kmeansCosCoords, kmeansCosOutput = kmeansClustering(pdBooksFeatures[CRITERIAS], \"cosine\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterPlotHelper(plotData):\n",
    "    fig, ax = plt.subplots(1, len(plotData), figsize=(14,6), dpi=120)\n",
    "    for i in range(len(plotData)):\n",
    "        if (plotData[i].axisLabels != []): ax[i].set(xlabel=plotData[i].axisLabels[0], ylabel=plotData[i].axisLabels[1])\n",
    "        scatter = ax[i].scatter(x=plotData[i].dataXY[0], y=plotData[i].dataXY[1], c=plotData[i].dataLabels, cmap=colors, s=10)\n",
    "        ax[i].legend(handles=scatter.legend_elements(num=3)[0], labels=CLASSES)\n",
    "        ax[i].set_title(plotData[i].title, loc='center', wrap=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculatePCA(booksData):\n",
    "    # Data Standardization\n",
    "    pdBooksFeaturesStandardized = (booksData - booksData.mean()) / booksData.std()\n",
    "    pdBooksFeaturesNormalized = pd.DataFrame(\n",
    "        data=preprocessing.normalize(pdBooksFeaturesStandardized), \n",
    "        columns=booksData.columns, \n",
    "        index=booksData.index\n",
    "    )\n",
    "    # PCA Calculations\n",
    "    pca = PCA(n_components=booksData.shape[1])\n",
    "    pca.fit(pdBooksFeaturesNormalized)\n",
    "    # PCA Results\n",
    "    booksFeaturesPCA = pd.DataFrame(\n",
    "        data=pca.components_.T,\n",
    "        columns=[f\"PC{i}\" for i in range(len(booksData.columns))],\n",
    "        index=booksData.columns\n",
    "    )\n",
    "    return pdBooksFeaturesNormalized, booksFeaturesPCA, pca\n",
    "\n",
    "\n",
    "# Calculate PCA of books features\n",
    "pdBooksFeaturesStandNorm, booksFeaturesPCA, pca = calculatePCA(pdBooksFeatures[CRITERIAS])\n",
    "# Display standardized data \n",
    "print(\"Standardized/Normalized books features matrix : \")\n",
    "display(pdBooksFeaturesStandNorm)\n",
    "\n",
    "# Plot all PCA axis and their contributions\n",
    "print(\"PC axis composition : \")\n",
    "display(booksFeaturesPCA)\n",
    "plt.figure(figsize=(8, 6), dpi=120)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.xlabel(\"Components\")\n",
    "plt.show()\n",
    "\n",
    "# Plot PCA (2 major axis)\n",
    "booksFeatures2PCA = pd.DataFrame(\n",
    "    data=pca.transform(pdBooksFeaturesStandNorm), \n",
    "    columns=[f\"PC{i}\" for i in range(len(pdBooksFeaturesStandNorm.columns))]\n",
    ")\n",
    "pcaSpaceXY = (booksFeatures2PCA[\"PC0\"], booksFeatures2PCA[\"PC1\"])\n",
    "pcaAxisLabels = [f\"PC0 - {math.floor(pca.explained_variance_ratio_[0]*100)}%\", f\"PC1 - {math.floor(pca.explained_variance_ratio_[1]*100)}%\"]\n",
    "\n",
    "kmeansEucli = PlotData(\n",
    "    inputData=kmeansEucliCoords,\n",
    "    dataXY=pcaSpaceXY,\n",
    "    dataLabels=kmeansEucliOutput.labels_, \n",
    "    title=\"K-Means clustering (PCA Space) \\n(Euclidean Distance)\",\n",
    "    axisLabels=pcaAxisLabels\n",
    " )\n",
    "kmeansCos = PlotData(\n",
    "    inputData=kmeansCosCoords,\n",
    "    dataXY=pcaSpaceXY,\n",
    "    dataLabels=kmeansCosOutput.labels_, \n",
    "    title=\"K-Means clustering (PCA Space) \\n(Cosine Similiarity)\",\n",
    "    axisLabels=pcaAxisLabels\n",
    ")\n",
    "scatterPlotHelper([kmeansEucli, kmeansCos])\n",
    "\n",
    "def displayKmeansClustering(booksData):\n",
    "    kmeansEucliCoords, kmeansEucliOutput = kmeansClustering(booksData, \"euclidean\")\n",
    "    kmeansCosCoords, kmeansCosOutput = kmeansClustering(booksData, \"cosine\")\n",
    "    pdBooksFeaturesStandNorm, _, pca = calculatePCA(booksData)\n",
    "    pcaSpace = pca.transform(pdBooksFeaturesStandNorm)\n",
    "    pcaSpaceXY = (pcaSpace[:,0], pcaSpace[:,1])\n",
    "    pcaAxisLabels = [f\"PC0 - {math.floor(pca.explained_variance_ratio_[0]*100)}%\", f\"PC1 - {math.floor(pca.explained_variance_ratio_[1]*100)}%\"]\n",
    "    kmeansEucli = PlotData(\n",
    "        inputData=kmeansEucliCoords,\n",
    "        dataXY=pcaSpaceXY,\n",
    "        dataLabels=kmeansEucliOutput.labels_, \n",
    "        title=\"K-Means clustering (PCA Space) \\n(Euclidean Distance)\",\n",
    "        axisLabels=pcaAxisLabels\n",
    "    )\n",
    "    kmeansCos = PlotData(\n",
    "        inputData=kmeansCosCoords,\n",
    "        dataXY=pcaSpaceXY,\n",
    "        dataLabels=kmeansCosOutput.labels_, \n",
    "        title=\"K-Means clustering (PCA Space) \\n(Cosine Similiarity)\",\n",
    "        axisLabels=pcaAxisLabels\n",
    "    )\n",
    "    scatterPlotHelper([kmeansEucli, kmeansCos])\n",
    "    return kmeansEucli, kmeansCos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 : Interprétation des résultats visuelles\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "def spectralClustering(booksData, distFunc):\n",
    "    eucli = lambda x, y: np.linalg.norm(x-y)\n",
    "    cosSim = lambda x, y: (x @ y.T) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    funcMap = {\"euclidean\": eucli, \"cosine\": cosSim}\n",
    "    # Standardize data before doing kmeans :\n",
    "    data = (booksData - booksData.mean()) / booksData.std()\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html\n",
    "    M = sp.spatial.distance.squareform(sp.spatial.distance.pdist(np.asarray(data), distFunc))\n",
    "    if (distFunc == \"cosine\"): M = 1 - M # SciPy uses distance cos which is equal to 1 - sim cos\n",
    "    eigenValues, eigenVectors = np.linalg.eig(M)\n",
    "    idx =  eigenValues.argsort()[::-1]\n",
    "    eigenValues = eigenValues[idx].real\n",
    "    eigenVectors = eigenVectors[:,idx].real\n",
    "    positiveCount = eigenValues[eigenValues > 0].size\n",
    "    P = np.stack(eigenVectors[:,0:positiveCount])\n",
    "    D = np.diag(eigenValues[0:positiveCount])\n",
    "    spectralDataMat = P @ np.sqrt(D)\n",
    "    kmeans = KMeans(n_clusters=NB_CLUSTERS, random_state=0, max_iter=1000).fit(spectralDataMat[:,0:2])\n",
    "    return spectralDataMat[:,0:2], kmeans\n",
    "\n",
    "def displaySpectralClustering(booksData, kmeansEucli, kmeansCos):\n",
    "    eucliCoords, spectralEucliOutput = spectralClustering(booksData, \"euclidean\")\n",
    "    cosCoords, spectralCosOutput =  spectralClustering(booksData, \"cosine\")\n",
    "    spectralEucli = PlotData(\n",
    "        inputData=eucliCoords,\n",
    "        dataXY=(eucliCoords[:,0],eucliCoords[:,1]), \n",
    "        dataLabels=spectralEucliOutput.labels_, \n",
    "        title=\"Spectral clustering (PCA Space of the Euclidean Distance Matrix) \\n(Euclidean distance)\"\n",
    "    )\n",
    "    spectralCos = PlotData(\n",
    "        inputData=cosCoords,\n",
    "        dataXY=(cosCoords[:,0],cosCoords[:,1]), \n",
    "        dataLabels=spectralCosOutput.labels_, \n",
    "        title=\"Spectral clustering (PCA space of the Cosine Similarity Matrix) \\n(Cosine Similarity)\"\n",
    "    )\n",
    "    scatterPlotHelper([spectralEucli, spectralCos])\n",
    "    spectralEucliPCASpace = PlotData(\n",
    "        inputData=eucliCoords,\n",
    "        dataXY=kmeansEucli.dataXY, \n",
    "        dataLabels=spectralEucliOutput.labels_, \n",
    "        title=\"Spectral clustering (PCA Space) \\n(Euclidean distance)\",\n",
    "        axisLabels=kmeansEucli.axisLabels\n",
    "    )\n",
    "    spectralCosPCASpace = PlotData(\n",
    "        inputData=cosCoords,\n",
    "        dataXY=kmeansCos.dataXY, \n",
    "        dataLabels=spectralCosOutput.labels_, \n",
    "        title=\"Spectral clustering (PCA Space) \\n(Cosine Similarity)\",\n",
    "        axisLabels=kmeansCos.axisLabels\n",
    "    )\n",
    "    scatterPlotHelper([spectralEucliPCASpace, spectralCosPCASpace])\n",
    "    return spectralEucli, spectralCos, spectralEucliPCASpace, spectralCosPCASpace\n",
    "\n",
    "spectralEucli, spectralCos, spectralEucliPCASpace, spectralCosPCASpace = displaySpectralClustering(\n",
    "    pdBooksFeatures[CRITERIAS], \n",
    "    kmeansEucli,\n",
    "    kmeansCos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "def getAccuracyMatrix(pdBooksFeatures, kmeansEucli, kmeansCos, spectralEucli, spectralCos):\n",
    "    bookClasses = pdBooksFeatures[[\"Class\"]].to_numpy(dtype=int).T[0]\n",
    "    perfMetric = np.array([\n",
    "        [\n",
    "            sk.metrics.silhouette_score(kmeansEucli.inputData, kmeansEucli.dataLabels, metric='euclidean'), \n",
    "            sk.metrics.adjusted_mutual_info_score(kmeansEucli.dataLabels, bookClasses), \n",
    "            sk.metrics.silhouette_score(kmeansCos.inputData, kmeansCos.dataLabels, metric=\"cosine\"), \n",
    "            sk.metrics.adjusted_mutual_info_score(kmeansCos.dataLabels, bookClasses), \n",
    "        ],\n",
    "        [\n",
    "            sk.metrics.silhouette_score(spectralEucli.inputData, spectralEucli.dataLabels, metric='euclidean'), \n",
    "            sk.metrics.adjusted_mutual_info_score(spectralEucli.dataLabels, bookClasses), \n",
    "            sk.metrics.silhouette_score(spectralCos.inputData, spectralCos.dataLabels, metric=\"cosine\"), \n",
    "            sk.metrics.adjusted_mutual_info_score(spectralCos.dataLabels, bookClasses), \n",
    "        ],\n",
    "    ])\n",
    "    return perfMetric\n",
    "\n",
    "def displayAccuracyMatrix(pdBooksFeatures, kmeansClustering, spectraClustering):\n",
    "    accMat = getAccuracyMatrix(\n",
    "        pdBooksFeatures, \n",
    "        kmeansClustering[0], \n",
    "        kmeansClustering[1], \n",
    "        spectraClustering[0], \n",
    "        spectraClustering[1]\n",
    "    )\n",
    "    display(pd.DataFrame(\n",
    "        data=accMat, \n",
    "        columns=[\"Silhouet (Euclidean)\", \"Mut. Info. (Euclidean)\", \"Silhouet (Cosine)\", \"Mut. Info. (Cosine)\",],\n",
    "        index=[\"K-means\", \"Spectral\"]\n",
    "    ))\n",
    "    return accMat\n",
    "\n",
    "_ = displayAccuracyMatrix(\n",
    "    pdBooksFeatures,\n",
    "    (kmeansEucli, kmeansCos), \n",
    "    (spectralEucli, spectralCos), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b-1) Selon vous quel serait le risque de prendre aléatoirement un sous-ensemble de données pour effectuer les tâches a-1) à a-5) ?**\n",
    "\n",
    "-> Puisque les 3 catégories ne sont pas distrbué d'une maniere uniforme si on prend aléatoirement un sous ensemble de données, on risque d'avoir des catégories qui seront forcément sous représenter voir même pas présent. Par conséquence, notre clustering sera forcément faux car on risque d'appliquer l'algorithme de clustering sur un ensemble de données qui appartient au même groupe (qui n'as pas de sens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q B-3\n",
    "def fullAnalysis(pdBooksFeatures):\n",
    "    kmeansEucli, kmeansCos = displayKmeansClustering(pdBooksFeatures[CRITERIAS])\n",
    "    spectralEucli, spectralCos, spectralEucliPCASpace, spectralCosPCASpace = displaySpectralClustering(\n",
    "        pdBooksFeatures[CRITERIAS], \n",
    "        kmeansEucli,\n",
    "        kmeansCos\n",
    "    )\n",
    "    displayAccuracyMatrix(\n",
    "        pdBooksFeatures, \n",
    "        (kmeansEucli, kmeansCos), \n",
    "        (spectralEucli, spectralCos)\n",
    "    )\n",
    "\n",
    "classesCount = [ (pdBooks.Class.values == i).sum() for i in range(0, 3) ]\n",
    "nbSamples = np.min(classesCount)\n",
    "pdBooksSampled = pdBooks.groupby('Class', group_keys=False).apply(lambda x: x.sample(nbSamples))\n",
    "display(pdBooksSampled)\n",
    "\n",
    "pdSampledBooksFeatures = getBookStats(pdBooksSampled)\n",
    "display(pdSampledBooksFeatures)\n",
    "displayDifferentClassesCount(pdSampledBooksFeatures)\n",
    "\n",
    "fullAnalysis(pdSampledBooksFeatures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "501834c540f8b330c04c1d43ec1666643355d13f80c9caf7e1080236b3fb263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
